# -*- coding: utf-8 -*-
"""ReadMyLips-Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13r5VwhTuqLrW2v0Odx4mrs_nfg8HLj75
"""

!pip install tensorflow==2.10

#tts instaltion-IBM_WATSON
!pip install ibm_watson

from google.colab import drive
drive.mount('/content/gdrive')

cd /content/gdrive/MyDrive/LipNet



"""# 0. Install and Import Dependencies"""

!pip list

!pip install opencv-python matplotlib imageio gdown

import os
import cv2 #preprocess videos libary
import tensorflow as tf #deep learning framework
import numpy as np #preprocess array of data
from typing import List
from matplotlib import pyplot as plt #post process data libary 
import imageio #convert numpy array to gif

tf.config.list_physical_devices('GPU')

#prevent memory expenational grow
physical_devices = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    pass

"""# 1. Build Data Loading Functions"""

import gdown #download data from drive

cd /content/gdrive/MyDrive/LipNet/

#download dataset from url link and extratced it to drive
#THE DATASET DOWNLOAD TAKES 10 MINUTES, SKIPPING IT IN THE VIDEO 
url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'
output = 'data.zip'
gdown.download(url, output, quiet=False)
gdown.extractall('data.zip')

def load_video(path:str) -> List[float]: 
    """
        Loads a video from the given path and preprocesses it.

        Parameters:
        path (str): The path to the video file.

        Returns:
        List[float]: A list of preprocessed video frames.

        This function loads a video from the given path using OpenCV's VideoCapture method and preprocesses each frame by converting it to grayscale and cropping the lip region from the frame. It then calculates the mean and standard deviation of the frames and returns the preprocessed frames as a list of floats.
        """
    cap = cv2.VideoCapture(path)
    frames = []
    #loop over each frame in video 
    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):
        ret, frame = cap.read()
        #convert each frame to grayscale
        frame = tf.image.rgb_to_grayscale(frame)
        #crop of lip region from video
        frames.append(frame[190:236,80:220,:])
    cap.release()
    
    mean = tf.math.reduce_mean(frames)#calculate mean
    std = tf.math.reduce_std(tf.cast(frames, tf.float32))#calc standerise
    return tf.cast((frames - mean), tf.float32) / std#return list of pre processing data of video

vocab = [x for x in "abcdefghijklmnopqrstuvwxyz'?!123456789 "]

#text preprocessing converting for the machine learning according to vocab
char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token="")#mapping chars to nums
num_to_char = tf.keras.layers.StringLookup(
    vocabulary=char_to_num.get_vocabulary(), oov_token="", invert=True#mapping nums to chars
)

print(
    f"The vocabulary is: {char_to_num.get_vocabulary()} "
    f"(size ={char_to_num.vocabulary_size()})"
)

char_to_num.get_vocabulary()

char_to_num(['a','b','c'])

num_to_char([1,2,3])

def load_alignments(path:str) -> List[str]:
  """
    Loads a file containing alignment information and preprocesses it.

    Parameters:
    path (str): The path to the file containing the alignment information.

    Returns:
    List[str]: A list of preprocessed alignment tokens.

    This function loads a file containing alignment information using Python's built-in file I/O functions. It then preprocesses the file by extracting the alignment tokens from each line, skipping any lines containing silent segments in the video. The tokens are then converted to numerical format and returned as a list of strings.
    """ 
  with open(path, 'r') as f: 
        lines = f.readlines() 
  tokens = []
    #loop over aligments line by line
  for line in lines:
        line = line.split()
        #if there is silent in video we skip this line
        if line[2] != 'sil':
            tokens = [*tokens,' ',line[2]]
  return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]

def load_data(path: str):
    """
    Loads the video frames and their corresponding alignments for a given file path.

    Parameters:
    path (str): The file path of the video file.

    Returns:
    Tuple[List[float], str]: A tuple containing a list of preprocessed video frames and the corresponding alignment string.

    This function loads the video frames and their corresponding alignments for a given file path. It first extracts the file name from the file path and uses it to locate the video file and its corresponding alignment file. It then calls the `load_video` function to preprocess the video frames and the `load_alignments` function to load the alignment data. The function returns a tuple containing the preprocessed video frames and the corresponding alignment string.
    """
    path = bytes.decode(path.numpy())
    file_name = path.split('/')[-1].split('.')[0]
    # File name splitting for windows
    #file_name = path.split('\\')[-1].split('.')[0]
    #/content/gdrive/MyDrive/LipNet/MyVideos

    #testing our own video 
    #video_path= os.path.join('MyVideos',f'{file_name}.mpg')
    #alignment_path=os.path.join('MyVideos',f'{file_name}.align')
    
    #testing on test_data
    video_path = os.path.join('data','s1',f'{file_name}.mpg')
    alignment_path = os.path.join('data','alignments','s1',f'{file_name}.align')
    frames = load_video(video_path) 
    alignments = load_alignments(alignment_path)
    
    return frames, alignments

test_path = './content/gdrive/MyDrive/LipNet/data/s1/bbal6n.mpg' #file path

tf.convert_to_tensor(test_path).numpy().decode('utf-8').split('/')[-1].split('.')[0] #split only the file name

frames, alignments = load_data(tf.convert_to_tensor(test_path))#preprocess data varibles of video and aligments

plt.imshow(frames[35]) #show one of the frames in array of frames

frames

alignments

tf.strings.reduce_join([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()]) #decoding bytes array of alignments

def mappable_function(path:str) ->List[str]:
    """
    Applies a Python function to a dataset path and returns the result.

    Parameters:
    path (str): The path to a dataset.

    Returns:
    List[str]: The result of applying a Python function to the dataset.

    This function applies a Python function called `load_data` to the dataset at the given path using TensorFlow's `tf.py_function` method. It returns the result of this function as a list of strings.
    """
    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))
    return result

"""# 2. Create Data Pipeline"""

from matplotlib import pyplot as plt

#create tensorflow data set of videos and alignments and mapping it as tensorflow data 
data = tf.data.Dataset.list_files('./data/s1/*.mpg')
data = data.shuffle(500, reshuffle_each_iteration=False)
data = data.map(mappable_function)#mapping dataset 
data = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))#batch of 2 videos and 2 alignments
data = data.prefetch(tf.data.AUTOTUNE)
# Added for split 
train = data.take(450)#train for first 450 samples
test = data.skip(450)#test for the rest of samples after the first 450

#size of videos test
len(test)

frames, alignments = data.as_numpy_iterator().next()

#batch size
len(frames)

sample = data.as_numpy_iterator()#iterator over dataset

val = sample.next(); val[0]#frames

imageio.mimsave('./animation.gif', val[0][0], fps=10)#convert numpy array to frames

# 0:videos, 0: 1st video out of the batch,  0: return the first frame in the video 
plt.imshow(val[0][0][35])

#example of tensor array output
tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])

"""# 3. Design the Deep Neural Network"""

#imports of neural network layers
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler

data.as_numpy_iterator().next()[0][0].shape#shape of data(75 frames, 46*140 pixels)

#build neural network architecture
model = Sequential()
model.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(256, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(75, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(TimeDistributed(Flatten()))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))

model.summary()

"""# Example of prediction before training

"""

yhat = model.predict(val[0]) #prediction

tf.strings.reduce_join([num_to_char(x) for x in tf.argmax(yhat[0],axis=1)])

tf.strings.reduce_join([num_to_char(tf.argmax(x)) for x in yhat[0]])

#the input shape of the preproccesed data 
model.input_shape

#the output shape of the preproccesed data 
model.output_shape

"""# 4. Setup Training Options and Train"""

def scheduler(epoch, lr):
  """
    Learning rate scheduler function that reduces the learning rate after a certain number of epochs.
    Parameters:
    epoch (int): The current epoch number.
    lr (float): The current learning rate.

    Returns:
    float: The new learning rate.
    This function is a simple learning rate scheduler that reduces the learning rate after 30 epochs. It takes the current epoch number and the current learning rate as input parameters and returns a new learning rate. If the current epoch is less than 30, the function returns the current learning rate. Otherwise, it reduces the learning rate by multiplying it with the mathematical constant e raised to the power of -0.1.
  """
  if epoch < 30:
      return lr
  else:
      return lr * tf.math.exp(-0.1)

def CTCLoss(y_true, y_pred):
    """
    Computes the Connectionist Temporal Classification (CTC) loss between the true and predicted labels.

    Parameters:
    y_true (Tensor): The true label tensor.
    y_pred (Tensor): The predicted label tensor.

    Returns:
    Tensor: The CTC loss tensor.

    This function computes the CTC loss between the true and predicted labels using the CTC batch cost function from the Keras backend. It takes in the true and predicted label tensors as input parameters and returns a tensor representing the CTC loss. The function also calculates the batch length, input length, and label length, which are required for the CTC batch cost function. 
    """
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")

    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    return loss

class ProduceExample(tf.keras.callbacks.Callback): 
   """
    Callback class that generates examples during training.

    Parameters:
    dataset (numpy iterator): The dataset iterator.

    This class is a callback function that generates examples during training. It takes in a dataset iterator as an input parameter during initialization. The class has an `on_epoch_end` method that is called at the end of each epoch. This method generates predictions for a batch of input data using the model, decodes the predictions using the CTC decoding function, and prints the original and predicted text for each input in the batch.
  """
   def __init__(self, dataset) -> None: 
            self.dataset = dataset.as_numpy_iterator()
    
   def on_epoch_end(self, epoch, logs=None) -> None:
      data = self.dataset.next()
      yhat = self.model.predict(data[0])
      decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()#decoding ctc model data
      for x in range(len(yhat)):           
         print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))
         print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))
         print('~'*100)

#compile model
model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)

#save model checkpoints
checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)

#dropdown learning rate for each epoch
schedule_callback = LearningRateScheduler(scheduler)

example_callback = ProduceExample(test)

#train the model
#model.fit(train, validation_data=test, epochs=100, callbacks=[checkpoint_callback, schedule_callback, example_callback])



"""# 5. Make a Prediction """

cd /content/gdrive/MyDrive/LipNet/models/

#download checkpoint of trained model
# url = 'https://drive.google.com/uc?id=1vWscXs4Vt0a_1IH1-ct2TCgXAZT-N3_Y'
# output = 'checkpoints.zip'
# gdown.download(url, output, quiet=False)
# gdown.extractall('checkpoints.zip', 'models')

#cd /content/gdrive/MyDrive/LipNet/models/models

#load model
#model.load_weights('/content/gdrive/MyDrive/LipNet/models/checkpoint')
model.load_weights('/content/gdrive/MyDrive/LipNet/models/models/test/checkpoint')

cd /content/gdrive/MyDrive/LipNet/

#iteartor of dataset numpy data
test_data = test.as_numpy_iterator() 

originalArray=[]
predictArray=[]
for i in range(49):
    sample = test_data.next()
    #prediction
    yhat = model.predict(sample[0])
    [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1][0]]]

    #decoding prediciton
    decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()
    [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded][0]

    original=[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1][0]]]
    last = [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]
    
    t_array = np.array(original)
    orignal_sentence = t_array[0].numpy().decode('utf-8')
    print('\nREAL TEXT\n','~'*100)
    print(orignal_sentence)
    print("\n")
    t_array = np.array(last)
    last_sentence = t_array[0].numpy().decode('utf-8')
    print('PREDICTED TEXT\n','~'*100)
    print(last_sentence)
    
    originalArray.append(orignal_sentence)
    predictArray.append(last_sentence)

# Get path of the dataset
import os
cwd = os.getcwd()
print(cwd)
path = 'data/alignments/s1/sran9s.align'
full_path = os.path.join(cwd, path)
print(full_path)

# sample = test_data.next()

#prediction
# yhat = model.predict(sample[0])

# print('REAL TEXT\n','~'*100)
# [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1]]]

#decoding prediciton
# decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()

# print('PREDICTED TEXT\n','~'*100)
# [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded][0]

# original=[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1][0]]]
# last = [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]

#In order to not waste time on testing and measure our algorithem we saved the result sentence in array 
# originalArray=['place white at q one soon', 'set red in a nine again', 'set blue at n five again', 'lay blue by e two please', 'place red with q five again', 'bin white with n nine again', 'place white at d two now', 'lay red in e zero please', 'lay white at f zero please', 'place red with d five soon', 'bin blue by f nine again', 'lay green by m four please', 'set red in u one again', 'place red by d two please', 'bin blue at s two please', 'bin blue in f zero please', 'set white with i nine soon', 'set red at n nine soon', 'bin white at g four now', 'lay blue in x four now', 'place white with k seven again', 'bin white with u zero now', 'set blue in t two now', 'lay green in f zero now', 'place green by k nine again']
# predictArray=['place white at one soon', 'set red in nine again', 'set blue at n five again', 'lay blue by two please', 'place red with f five again', 'bin white with n nine again', 'place white at two now', 'lay red in z zero please', 'lay white at zero please', 'place red with five soon', 'bin blue by nine again', 'lay green by four please', 'set red in u one again', 'place red by two please', 'bin blue at s two please', 'bin blue in zero please', 'set white with nine soon', 'set red at n nine soon', 'bin white at four now', 'lay blue in x four now', 'place white with seven again', 'bin white with zero now', 'set blue in t two now', 'lay green in z zero now', 'place green by nine again', 'place green by nine again']

#decoding original text and output from tensorFlow array
# t_array = np.array(original)
# orignal_sentence = t_array[0].numpy().decode('utf-8')
# print('REAL TEXT\n','~'*100)
# print(orignal_sentence)
# print("\n")
# t_array = np.array(last)
# last_sentence = t_array[0].numpy().decode('utf-8')
# print('PREDICTED TEXT\n','~'*100)
# print(last_sentence)

# originalArray.append(orignal_sentence)
# predictArray.append(last_sentence)
# print(predictArray)

"""#CER-CHARACTER ERROR RATE"""

def calculate_cer(ref,hyp):
  totalCer=0
  # originalChars=ref.split()
  # predictedChars=hyp.split()
  for i in range (len(originalChars)):
      totalCer=totalCer+calculate_wer(ref[i],hyp[i])
  return totalCer

# def countChars(originalText):
#     characters=originalText.split()
#     return len(characters)

# import nltk

# def calculate_cer(originalText, predictText):
#   for index in zip(originalText, predictText):
#     originalText = originalText[i].split()
#     predictText = predictText[i].split()
#     index=0
#     # Use NLTK edit_distance to calculate CER
#     totalCER = 0
#     for characters in zip(originalText, predictText):
#         cer = nltk.edit_distance(characters[0], characters[1])          
#         totalCER += cer

#     return totalCER

"""# WER-WORD ERROR RATE"""

def calculate_wer(ref, hyp ,debug=False):
    r = ref.split()
    h = hyp.split()
    #costs will holds the costs, like in the Levenshtein distance algorithm
    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]
    # backtrace will hold the operations we've done.
    # so we could later backtrace, like the WER algorithm requires us to.
    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]
 
    OP_OK = 0
    OP_SUB = 1
    OP_INS = 2
    OP_DEL = 3
    DEL_PENALTY = 1
    INS_PENALTY = 1
    SUB_PENALTY = 1
    
    # First column represents the case where we achieve zero
    # hypothesis words by deleting all reference words.
    for i in range(1, len(r)+1):
        costs[i][0] = DEL_PENALTY*i
        backtrace[i][0] = OP_DEL
    
    # First row represents the case where we achieve the hypothesis
    # by inserting all hypothesis words into a zero-length reference.
    for j in range(1, len(h) + 1):
        costs[0][j] = INS_PENALTY * j
        backtrace[0][j] = OP_INS
    
    # computation
    for i in range(1, len(r)+1):
        for j in range(1, len(h)+1):
            if r[i-1] == h[j-1]:
                costs[i][j] = costs[i-1][j-1]
                backtrace[i][j] = OP_OK
            else:
                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1
                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1
                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1
                 
                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)
                if costs[i][j] == substitutionCost:
                    backtrace[i][j] = OP_SUB
                elif costs[i][j] == insertionCost:
                    backtrace[i][j] = OP_INS
                else:
                    backtrace[i][j] = OP_DEL
                 
    # back trace though the best route:
    i = len(r)
    j = len(h)
    numSub = 0
    numDel = 0
    numIns = 0
    numCor = 0
    if debug:
      #  print("OP\tREF\tHYP")
        lines = []
    while i > 0 or j > 0:
        if backtrace[i][j] == OP_OK:
            numCor += 1
            i-=1
            j-=1
            if debug:
                lines.append("OK\t" + r[i]+"\t"+h[j])
        elif backtrace[i][j] == OP_SUB:
            numSub +=1
            i-=1
            j-=1
            if debug:
                lines.append("SUB\t" + r[i]+"\t"+h[j])
        elif backtrace[i][j] == OP_INS:
            numIns += 1
            j-=1
            if debug:
                lines.append("INS\t" + "****" + "\t" + h[j])
        elif backtrace[i][j] == OP_DEL:
            numDel += 1
            i-=1
            if debug:
                lines.append("DEL\t" + r[i]+"\t"+"****")
    if debug:
        lines = reversed(lines)
        for line in lines:
            print(line)
        print("#cor " + str(numCor))
        print("#sub " + str(numSub))
        print("#del " + str(numDel))
        print("#ins " + str(numIns))
    # return (numSub + numDel + numIns)
    wer_result = numSub + numDel + numIns
    return wer_result

wer=0
cer=0
lastwer=0
lastcer=0
counterwords=0
counterChars=0
originalChars=[]
predictChars=[]
for i in range(len(originalArray)): 
  words=originalArray[i].split() #[set,white,at,blue]
  counterwords=counterwords+len(words)
  wordspredict=predictArray[i].split()
  for j in range (min(len(words), len(wordspredict))):
      cer=cer+calculate_wer(words[j],wordspredict[j])
      counterChars=counterChars+len(words[j])

for index in range(len(originalArray)):
    wer=wer+calculate_wer(originalArray[index],predictArray[index])
    
lastwer=(int)((wer/counterwords)* 100)
lastcer=(int)((cer/counterChars)* 100)
#presentage of WER
print("---------------------------------------\n")
print("Measurements:\n")
print("WER:", lastwer,"%","CER: ",lastcer,"%" )
print("---------------------------------------\n")

"""# Test on a Video"""

cd /content/gdrive/MyDrive/LipNet/

#file path of my own video
filePath='/content/gdrive/MyDrive/LipNet/MyVideos/swab6a.mpg'

#file path of video from dataset
filePath='/content/gdrive/MyDrive/LipNet/data/s1/bbal8p.mpg'
#filePath='/content/gdrive/MyDrive/LipNet/data/s1/pgbk8p.mpg' 
#arrange(483000,without changing the data shape)-> numpy array(0,482000), reshape(75,46,140,1,without )

sample = load_data(tf.convert_to_tensor(filePath))

filename=filePath.split('/')[-1].split('.')[0] #split only the file name

yhat = model.predict(tf.expand_dims(sample[0], axis=0))

decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()

print('REAL TEXT\n','~'*100)
[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1]]]

print('PREDICTED TEXT\n','~'*100)
[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]

original=[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1]]]

t_array = np.array(original)
orignal_sentence = t_array[0].numpy().decode('utf-8')
print(orignal_sentence)

last = [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]

imageio.mimsave('/content/gdrive/MyDrive/LipNet/animation/animation'+filename+'.gif', sample[0], fps=10)#convert numpy array to frames and create gif

# Display GIF in Jupyter, CoLab, IPython
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from IPython import display
from IPython.display import Image
from pathlib import Path

gifPath = Path('/content/gdrive/MyDrive/LipNet/animation/animation'+filename+'.gif') 
with open(gifPath,'rb') as f:
    display.Image(data=f.read(), format='png',width=350)

#OUTPUT SENTENCE
t_array = np.array(last)
last_sentence = t_array[0].numpy().decode('utf-8')
print('     ','  ',last_sentence,'   ')

"""# 1. Authenticate"""

from ibm_watson import TextToSpeechV1
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator

apikey = 'jwv3z0rLKVAG4-2MSTOwAQlsnoeA_yubgLwLOaxcurFH'
url = 'https://api.au-syd.text-to-speech.watson.cloud.ibm.com/instances/07d2be72-5a3c-4da1-85f3-cbab3a60795f'

# Setup Service
authenticator = IAMAuthenticator(apikey)
tts = TextToSpeechV1(authenticator=authenticator)
tts.set_service_url(url)

"""## 2. Convert with A Basic Language Model"""

with open('/content/gdrive/MyDrive/LipNet/audio/speech'+'_'+filename+'.wav', 'wb') as audio_file:
    res = tts.synthesize(last_sentence, accept='audio/wav', voice='en-US_KevinV3Voice').get_result()
    audio_file.write(res.content)

#play sound of predicted text
from IPython.display import Audio
from IPython.display import display
sound_file = '/content/gdrive/MyDrive/LipNet/audio/speech'+'_'+filename+'.wav'
wn = Audio(sound_file, autoplay=True)
display(wn)

cd /content/gdrive/MyDrive/LipNet/audio/

#show output graph of speech synthesis
import librosa
def print_plot_play(fileName, text=''):
    x, Fs = librosa.load(fileName, sr=None)
    print('%s Fs = %d, x.shape = %s, x.dtype = %s' % (text, Fs, x.shape, x.dtype))
    plt.figure(figsize=(10, 5))
    plt.plot(x, color='blue')
    plt.xlim([0, x.shape[0]])
    plt.xlabel('Time (samples)')
    plt.ylabel('Amplitude')
    plt.tight_layout()
    plt.show()

"""# Outpot of Speech synthesis and check the quality"""

print_plot_play(sound_file)

"""## 3. Reading from a File """

# with open('churchill.txt', 'r') as f:
#     text = f.readlines()

# text = [line.replace('\n','') for line in text]

# text = ''.join(str(line) for line in text)

# with open('./winston.mp3', 'wb') as audio_file:
#     res = tts.synthesize(text, accept='audio/mp3', voice='en-GB_JamesV3Voice').get_result()
#     audio_file.write(res.content)